{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebooks Included\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Basic/Exploratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Distributions of SIE and SIE anom in models and observations (<code>explore_SIE_models_vs_obs_variability.ipynb</code>)\n",
    " 2. Calculate anomalous SIE (we remove climatology) for models and observations.  Climatology for S2S model output depends on both lead time and day of year (<code>create_model_SIE_anoms.ipynb</code>, <code>craete_obs_SIE_anoms.ipynb</code>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Model bias (model SIE - obs SIE) as a function of lead time, month (<code>calculate_S2S_model_bias.ipynb</code>)\n",
    "     *  How bad are model biases on day 1? After 1 week? <i>A: It depends, some models are pretty bad</i>\n",
    "     *  Do model biases drift at longer lead times? <i>A: not really...models that start biased low tend to stay biased low</i>\n",
    "     *  What is the seasonal (and regional) variability of these biases? <i>A: substantial; biases are often greatest in the summer though!</i>\n",
    " 2. Is there a consistent relationship between biases in the mean SIE and the variability of SIE? (<code>calculate_S2S_model_bias_vs_month_mean_SD.ipynb</code>)\n",
    "     *  <i>No</i>\n",
    " 3. Do models get their own VRILES? Do they get them in approximately the right seasons? (<code>calc_S2S_models_VRILE_counts.ipynb</code>)\n",
    "     *  Broadly, yes, though models are of course not perfect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Predictability\n",
    " 1.  Is SIE on extreme days (days in which a VRILE occured in the observations) more predictable than sea ice on typical days? (<code>calculate_RMSE_MAE_S2S_models_VRILE_vs_noVRILE_days.ipynb</code>)\n",
    "     * Typically, error is higher on VRILE days, though this varies with season/region to an extent\n",
    "     * Models have large errors at the beginning of their forecast periods, which can persist\n",
    "\n",
    " 2.  How sensitive are our results to our choice of observations (re-run <code>calculate_RMSE_MAE_S2S_models_VRILE_vs_noVRILE_days.ipynb</code> with NASA Team instead of NASA Bootstrap)\n",
    "     * The conclusions are not sensistive to our choice of observations; of course, exact values of RMSE will change somewhat but big picture (sea ice is less predictable on VRILE days) does not change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b.  Statistical Model\n",
    " 1.  Create a statistical benchmark forecast using a damped anomaly (persistence + climatology) forecast created from observations (<code>create_SIE_clim_benchmark.ipynb</code>)\n",
    " 2.  Evaluate prediction skill of statistical model (<code>damped_anom_forecast_RMSE.ipynb</code>)\n",
    " 3.  Compare different methods of creating statistical model (rolling mean climatology vs static; NASA Team vs Bootstrap) (run <code>create_SIE_clim_benchmark.ipynb</code> and <code>damped_anom_forecast_RMSE.ipynb</code> with different obs; different climatology method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Uncertainty\n",
    " 1.  How can we be sure that our VRILE days are actually distinct from any other random sample of 5%? (<i> bootstrapping code</i>)\n",
    "     * Bootstrapping code (random samples of 5/10% of days) to calculate a series of lead-dependent RMSEs\n",
    "     * Evaluation of significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Error Growth\n",
    " 1. If error is higher on VRILE days, how long does this last? (<i> error_growth</i>)\n",
    "     * It actually lasts for a while\n",
    " 2. Shifting our predictions to start on/before VRILE events (<i> error growth</i>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  Plotting\n",
    " 1.  Plotting model bias as a function of region, forecast month, for a specific lead period. (<code>plot_S2S_model_bias.ipynb</code>)\n",
    " 2.  Plotting model VRILE counts as a function of region, forecast month for a specific lead period (<i>VRILE count plot code</i>)\n",
    " 3.  Do the dynamical models show skill compared to the statistical forecast model? (<i>12 panel plots code</i>)\n",
    "     * The statistical forecast model is pretty skillful overall.  HOWEVER, the statistical model is not actually not great with extreme events.  So while the statistical model is still pretty good with forecasting mean-state-like conditions, it's actually pretty bad at forecasting VRILEs in a lot of cases\n",
    "     * Basic plot just showing RMSE for all days for each model, region; year-round \n",
    "     * Comparing ratios of RMSE on VRILE/nonVRILE days for each model, region; also comparing seasons\n",
    " 4.  Plotting error growth\n",
    " 5.  Plotting region mask\n",
    " #### Plots for Supplemental Material\n",
    " 5.  Plotting different damped anomaly forecasts\n",
    " 6.  Plotting RMSE for NASA Team vs NASA Bootstrap\n",
    " 7.  Plotting RMSE for VRILEs at 5% vs 10%\n",
    " 8.  Bootstrapping plot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sea_ice_variability_S2S",
   "language": "python",
   "name": "sea_ice_variability_s2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
